# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-dVhrpiSxpoY-MJG39COntKVTKpmzl88

##1. **Introduction**
Credit card fraud is a widespread issue that occurs globally every day. However, credit card companies have made notable advancements in detecting and preventing fraudulent transactions to safeguard their customers. This dataset contains credit card transaction data from Europe, recorded over two days in September 2013.

The objective of this notebook is to identify fraudulent transactions and predict when they may occur.

##2. **Importing Libraries and Data**
"""

# Importing essential libraries for data manipulation, machine learning, and visualization
import pandas as pd  # For data manipulation and loading datasets
import numpy as np   # For numerical operations
import matplotlib.pyplot as plt  # For data visualization
import seaborn as sns  # For advanced data visualization

from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from sklearn.preprocessing import StandardScaler  # For scaling features
from sklearn.linear_model import LogisticRegression  # For Logistic Regression model
from sklearn.tree import DecisionTreeClassifier  # For Decision Tree model
from sklearn.ensemble import RandomForestClassifier  # For Random Forest model
from sklearn.metrics import classification_report, confusion_matrix  # For evaluating model performance
from imblearn.over_sampling import SMOTE  # For handling class imbalance using SMOTE

df_train_original = pd.read_csv('/content/fraudTrain.csv') #Loading the training data
df_train_original.drop(df_train_original.columns[0], axis=1, inplace=True) #Drop the first column of the DataFrame

df_test_original = pd.read_csv('/content/fraudTest.csv') #Loading the testing data
df_test_original.drop(df_test_original.columns[0], axis=1, inplace=True) #Drop the first column of the DataFrame

# Define the proportion of the sample size we want
sample_size = 0.15

# Performing stratified sampling
df, _ = train_test_split(df_train_original, test_size=1-sample_size, stratify=df_train_original['is_fraud'], random_state=42)

# Displaying the sampled DataFrame
df.shape

"""##3. **EDA (Exploratory Data Analysis)**

###3.1 Handling Missing Values
"""

# Check for missing values
print(df_train_original.isnull().sum())
print(df_test_original.isnull().sum())

"""We can see that there are no missing values in the datasets.

###3.2 Data Summary and Data Balance Check
"""

# Summary statistics of the training data
print(df_train_original.describe())

# Check the distribution of the target variable (assuming 'Class' is the target column)
sns.countplot(x='is_fraud', data=df)
plt.show()

#Data Balance Check
df['is_fraud_cat'] = df['is_fraud'].apply(lambda x: "Fraud" if x==1 else "No Fraud")

is_fraud_values = df['is_fraud_cat'].value_counts()

plt.figure(figsize=(7,7))
plt.pie(is_fraud_values, labels=is_fraud_values.index, autopct='%1.1f%%', startangle=90, colors = sns.color_palette("Set2", n_colors=len(is_fraud_values)))
plt.title('Percentage of Fraudulent vs Non-fraudulent transactions')

"""From the diagrams we see that the data exhibits a significant class imbalance, indicating a strong skew in the distribution of the classes.

###3.3 Fraud by Transaction Categories
"""

sns.countplot(x="category", data=df[df['is_fraud_cat']=="Fraud"], palette="Set2", hue = 'category')

plt.title('Fraudulent Transactions by Category')
plt.xlabel('Transaction Category')
plt.ylabel('Count of Fraud Instance')
plt.xticks(rotation=45, ha = 'right')
plt.figure(figsize=(10, 24))
plt.show()

"""###3.4 Fraud by Jobs"""

df[df['is_fraud_cat']=="Fraud"]["job"].value_counts(sort=True,ascending=False).head(10).plot(kind="bar",x='Jobs', y=df['is_fraud_cat']=="Fraud", color=['lightblue', 'lightgreen', 'lightcoral', 'orange', 'lightpink','cornflowerblue', 'green', 'coral', 'darkorange', 'orchid'])
plt.title("Credit Card Frauds by Job")
plt.figure(figsize=(10, 24))
plt.show()

"""##4. **Data Pre-processing**

###4.1. Handling Duplicate Values
"""

# Create a copy of the original DataFrame 'df' to avoid modifying the original data
df2 = df.copy()

# Remove duplicate rows from 'df2', keeping only unique rows
# The 'inplace=True' argument ensures the changes are made directly to 'df2'
df2.drop_duplicates(inplace=True)

# Print a message confirming that the duplicates have been dropped
print("Duplicated Values Dropped")

"""###4.2. Transforming Variables
Transforming the gender variable to be binary
"""

def gender_transform(x):
    if x=='F':
        return 1
    if x=='M':
        return 0
df2['gender'] = df2['gender'].transform(gender_transform)

"""###4.3. Dummy Variables
We will examine the categorical data and the number of unique values in each to decide which ones to keep for creating dummy variables.
"""

df3 = df2.drop(['unix_time','trans_date_trans_time','cc_num','trans_num','street','dob','city','merchant','job','last','first','state','is_fraud_cat'],axis = 1)

# Convert categorical columns to dummy variables (binary columns) and store them in 'df3_dummies'
df3_dummies = pd.get_dummies(df3.select_dtypes('object'), dtype=int)

# Drop the original categorical columns and concatenate the dummy variables to 'df3'
df4 = pd.concat([df3.drop(df3.select_dtypes('object').columns, axis=1), df3_dummies], axis=1)

# Print the shape of the new DataFrame to check the number of rows and columns
print(df4.shape)

# Display the first few rows of the new DataFrame to inspect the result
df4.head()

"""###4.4 Stratified Train/Test split
We need to generate a training / validation dataset split that will keep the same percentages of classes in each split.
"""

X = df4.reset_index(drop='index').drop('is_fraud', axis=1)

y = df4.reset_index(drop='index')['is_fraud']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3, random_state = 42)

"""##5. **Baseline Models (no imbalance resolving)**
We will build some baseline models (Logistic regression, Decision Trees and Random Forests model) so we can have a reference in how the model performs if we left the data as it is.

###5.1. **Logistic Regression**
"""

#Import Required Libraries

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Initialize the Logistic Regression model
log_reg = LogisticRegression(random_state=42, max_iter=1000)

# Train the model
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred_base = log_reg.predict(X_test)

# Calculate and print performance metrics
accuracy = accuracy_score(y_test, y_pred_base)
precision = precision_score(y_test, y_pred_base)
recall = recall_score(y_test, y_pred_base)
f1 = f1_score(y_test, y_pred_base)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

# Display the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_base)
print("Confusion Matrix:\n", conf_matrix)

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred_base))

#creating an overall performance table
cm = confusion_matrix(y_test, y_pred_base)

lr_baseline_Recall = recall_score(y_test, y_pred_base)
lr_baseline_Precision = precision_score(y_test, y_pred_base)
lr_baseline_f1 = f1_score(y_test, y_pred_base)
lr_baseline_accuracy = accuracy_score(y_test, y_pred_base)

ndf = [(lr_baseline_Recall, lr_baseline_Precision, lr_baseline_f1, lr_baseline_accuracy)]

lr_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
lr_score.insert(0, 'Logistic Regression performed with', 'Original (Imbalanced Dataset)')
lr_score

"""Note: Recall, precision, and F1-score are currently 0. This issue likely arises due to the class imbalance in the dataset, where the model predicts only the majority class (non-fraudulent transactions).

###5.2. **Decision Tree**
"""

#Import Required Libraries

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialize the Decision Tree classifier
dt_model = DecisionTreeClassifier(random_state=42)

# Fit the model on the training data
dt_model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = dt_model.predict(X_test)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

#creating an overall performance table
cm = confusion_matrix(y_test, y_pred)

dt_baseline_Recall = recall_score(y_test, y_pred)
dt_baseline_Precision = precision_score(y_test, y_pred)
dt_baseline_f1 = f1_score(y_test, y_pred)
dt_baseline_accuracy = accuracy_score(y_test, y_pred)

ndf = [(dt_baseline_Recall, dt_baseline_Precision, dt_baseline_f1, dt_baseline_accuracy)]

dt_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
dt_score.insert(0, 'Decision Tree performed with', 'Original (Imbalanced Dataset)')
dt_score

"""###5.3. **Random Forest**"""

#Import the required libraries

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialize the Random Forest classifier
rf_model = RandomForestClassifier(random_state=42, n_estimators=100)

# Fit the model on the training data
rf_model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred1 = rf_model.predict(X_test)

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred1))

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred1))

#creating an overall performance table
cm = confusion_matrix(y_test, y_pred1)

rf_baseline_Recall = recall_score(y_test, y_pred1)
rf_baseline_Precision = precision_score(y_test, y_pred1)
rf_baseline_f1 = f1_score(y_test, y_pred1)
rf_baseline_accuracy = accuracy_score(y_test, y_pred1)

ndf = [(rf_baseline_Recall, rf_baseline_Precision, rf_baseline_f1, rf_baseline_accuracy)]

rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
rf_score.insert(0, 'Random Forest performed with', 'Original (Imbalanced Dataset)')
rf_score

"""Here, we are getting higher Precision, F1 Score, and Accuracy. So we can use the Random Forest model for fraud detection as this model can handle Linear as well as Non-linear relations efficiently.

In case of the lower Recall value, we have to balance the dataset using undersampling or SMOTE to get a higher value.

##6. **Handling the Imbalanced Data**

###6.1. Undersampling
"""

#Initialize the model

from imblearn.under_sampling import RandomUnderSampler
# defining undersampling method
rus = RandomUnderSampler(random_state=42)

rf_undersample = RandomForestClassifier(n_estimators=200)
X_train_undersample, y_train_undersample = rus.fit_resample(X_train, y_train)

#Fitting the model

rf_undersample.fit(X_train_undersample, y_train_undersample)
y_pred_undersample = rf_undersample.predict(X_test)

#Performance & Accuracy

print("Performance with Random Undersampling:")
print(classification_report(y_test, y_pred_undersample))

#creating an overall performance table
cm = confusion_matrix(y_test, y_pred_undersample)

undersample_rf_Recall = recall_score(y_test, y_pred_undersample)
undersample_rf_Precision = precision_score(y_test, y_pred_undersample)
undersample_rf_f1 = f1_score(y_test, y_pred_undersample)
undersample_rf_accuracy = accuracy_score(y_test, y_pred_undersample)

ndf_over = [(undersample_rf_Recall, undersample_rf_Precision, undersample_rf_f1, undersample_rf_accuracy)]

undersample_rf_score = pd.DataFrame(data = ndf_over, columns=['Recall','Precision','F1 Score', 'Accuracy'])
undersample_rf_score.insert(0, 'Random Forest performed with', 'Undersampling')
undersample_rf_score

"""###6.2. SMOTE (Synthetic Minority Over-sampling Technique)"""

#Initialize the model

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
rf_smote = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

#Fitting the model

rf_smote.fit(X_train_smote,y_train_smote)
y_pred_smote = rf_smote.predict(X_test)

#Performance and Accuracy

print("Performance with SMOTE:")
print(classification_report(y_test, y_pred_smote))

#creating an overall performance table
cm_smote = confusion_matrix(y_test, y_pred_smote)
rf_smote_Recall = recall_score(y_test, y_pred_smote)
rf_smote_Precision = precision_score(y_test, y_pred_smote)
rf_smote_f1 = f1_score(y_test, y_pred_smote)
rf_smote_accuracy = accuracy_score(y_test, y_pred_smote)

ndf = [(rf_smote_Recall, rf_smote_Precision, rf_smote_f1, rf_smote_accuracy)]

rf_smote_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])
rf_smote_score.insert(0, 'Random Forest performed with', 'SMOTE')
rf_smote_score

"""###6.3. ***Performance Summary***"""

#creating an overall performance table
rf_sampling_scores = pd.concat([rf_score, undersample_rf_score, rf_smote_score], axis=0)
rf_sampling_scores

"""For our fraud detection model, we chose **undersampling** despite its lower precision (0.117) because it achieves the highest recall of 0.934, meaning it catches 93.4% of fraud cases. While this creates a more "paranoid" model that may generate more false alarms, in fraud detection, missing fraudulent transactions (false negatives) is far more costly than having false positives, which can be quickly verified.

 This approach is preferred as the cost of investigating suspicious transactions is minimal compared to the potential losses from undetected fraud.

##7. **Applying the best model**
As mentioned earlier, we will continue with Undersampling
"""

#Showing the accuracy score using SMOTE

accuracy = accuracy_score(y_test, y_pred_undersample)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

#Print the predicted and actual class names

# remapping the classes
class_names = {1:'Fraud',0:'No Fraud'}
predicted_classes = [class_names[label] for label in y_pred_undersample]

# Mapping true labels to custom class names
class_map = [class_names[label] for label in y_test]

# Step 10: Print the predicted and actual class names for the first 5 test samples
for i in range(20):
    print(f"Sample {i+1}: True Class: {class_map[i]}, Predicted Class: {predicted_classes[i]}")